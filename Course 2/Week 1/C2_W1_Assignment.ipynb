{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation with Amazon SageMaker processing job and Feature Store\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab you will start with the raw [Women's Clothing Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset and prepare it to train a BERT-based natural language processing (NLP) model. The model will be used to classify customer reviews into positive (1), neutral (0) and negative (-1) sentiment.\n",
    "\n",
    "You will convert the original review text into machine-readable features used by BERT. To perform the required feature transformation you will configure an Amazon SageMaker processing job, which will be running a custom Python script.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [1. Configure the SageMaker Feature Store](#c2w1-1.)\n",
    "  - [1.1. Configure dataset](#c2w1-1.1.)\n",
    "  - [1.2. Configure the SageMaker feature store](#c2w1-1.2.)\n",
    "    - [Exercise 1](#c2w1-ex-1)\n",
    "- [2. Transform the dataset](#c2w1-2.)\n",
    "    - [Exercise 2](#c2w1-ex-2)\n",
    "    - [Exercise 3](#c2w1-ex-3)\n",
    "- [3. Query the Feature Store](#c2w1-3.)\n",
    "  - [3.1. Export training, validation, and test datasets from the Feature Store](#c2w1-3.1.)\n",
    "    - [Exercise 4](#c2w1-ex-4)\n",
    "  - [3.2. Export TSV from Feature Store](#c2w1-3.2.)\n",
    "  - [3.3. Check that the dataset in the Feature Store is balanced by sentiment](#c2w1-3.3.)\n",
    "    - [Exercise 5](#c2w1-ex-5)\n",
    "    - [Exercise 6](#c2w1-ex-6)\n",
    "    - [Exercise 7](#c2w1-ex-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch==1.6.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2021.5.25  |       h06a4308_1         112 KB\n",
      "    certifi-2021.5.30          |   py37h06a4308_0         139 KB\n",
      "    conda-4.10.3               |   py37h06a4308_0         2.9 MB\n",
      "    cudatoolkit-10.2.89        |       hfd86e86_1       365.1 MB\n",
      "    ninja-1.10.2               |       hff7bd54_1         1.4 MB\n",
      "    pytorch-1.6.0              |py3.7_cuda10.2.89_cudnn7.6.5_0       537.7 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       907.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.2.89-hfd86e86_1\n",
      "  ninja              pkgs/main/linux-64::ninja-1.10.2-hff7bd54_1\n",
      "  pytorch            pytorch/linux-64::pytorch-1.6.0-py3.7_cuda10.2.89_cudnn7.6.5_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2021.5.25-h06a4308_1\n",
      "  certifi            conda-forge::certifi-2020.12.5-py37h8~ --> pkgs/main::certifi-2021.5.30-py37h06a4308_0\n",
      "  conda              conda-forge::conda-4.10.1-py37h89c186~ --> pkgs/main::conda-4.10.3-py37h06a4308_0\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!conda install -q -y pytorch==1.6.0 -c pytorch\n",
    "!pip install --disable-pip-version-check -q transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.'></a>\n",
    "# 1. Configure the SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.1.'></a>\n",
    "### 1.1. Configure dataset\n",
    "The raw dataset is in the public S3 bucket. Let's start by specifying the S3 location of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://dlai-practical-data-science/data/raw/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://dlai-practical-data-science/data/raw/'\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the files in the S3 bucket (in this case it will be just one file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-30 02:21:06    8457214 womens_clothing_ecommerce_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-1.2.'></a>\n",
    "### 1.2. Configure the SageMaker feature store\n",
    "\n",
    "As the result of the transformation, in addition to generating files in S3 bucket, you will also save the transformed data in the **Amazon SageMaker Feature Store** to be used by others in your organization, for example. \n",
    "\n",
    "To configure a Feature Store you need to setup a **Feature Group**. This is the main resource containing all of the metadata related to the data stored in the Feature Store. A Feature Group should contain a list of **Feature Definitions**. A Feature Definition consists of a name and the data type. The Feature Group also contains an online store configuration and an offline store configuration controlling where the data is stored. Enabling the online store allows quick access to the latest value for a record via the [GetRecord API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_feature_store_GetRecord.html). The offline store allows storage of the data in your S3 bucket. You will be using the offline store in this lab.\n",
    "\n",
    "Let's setup the Feature Group name and the Feature Store offline prefix in S3 bucket (you will use those later in the lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group name: reviews-feature-group-1625365556\n",
      "Feature store offline prefix in S3: reviews-feature-store-1625365556\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "feature_group_name = 'reviews-feature-group-' + str(timestamp)\n",
    "feature_store_offline_prefix = 'reviews-feature-store-' + str(timestamp)\n",
    "\n",
    "print('Feature group name: {}'.format(feature_group_name))\n",
    "print('Feature store offline prefix in S3: {}'.format(feature_store_offline_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking two features from the original raw dataset (`Review Text` and `Rating`), you will transform it preparing to be used for the model training and then to be saved in the Feature Store. Here you will define the related features to be stored as a list of `FeatureDefinition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions= [\n",
    "    # unique ID of the review\n",
    "    FeatureDefinition(feature_name='review_id', feature_type=FeatureTypeEnum.STRING), \n",
    "    # ingestion timestamp\n",
    "    FeatureDefinition(feature_name='date', feature_type=FeatureTypeEnum.STRING),\n",
    "    # sentiment: -1 (negative), 0 (neutral) or 1 (positive). It will be found the Rating values (1, 2, 3, 4, 5)\n",
    "    FeatureDefinition(feature_name='sentiment', feature_type=FeatureTypeEnum.STRING), \n",
    "    # label ID of the target class (sentiment)\n",
    "    FeatureDefinition(feature_name='label_id', feature_type=FeatureTypeEnum.STRING),\n",
    "    # reviews encoded with the BERT tokenizer\n",
    "    FeatureDefinition(feature_name='input_ids', feature_type=FeatureTypeEnum.STRING),\n",
    "    # original Review Text\n",
    "    FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\n",
    "    # train/validation/test label\n",
    "    FeatureDefinition(feature_name='split_type', feature_type=FeatureTypeEnum.STRING)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-1'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Create the feature group using the feature definitions defined above.\n",
    "\n",
    "**Instructions:** Use the `FeatureGroup` function passing the defined above feature group name and the feature definitions.\n",
    "\n",
    "```python\n",
    "feature_group = FeatureGroup(\n",
    "    name=..., # Feature Group name\n",
    "    feature_definitions=..., # a list of Feature Definitions\n",
    "    sagemaker_session=sess # SageMaker session\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-1625365556', sagemaker_session=<sagemaker.session.Session object at 0x7ffa30373810>, feature_definitions=[FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='sentiment', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='review_body', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='split_type', feature_type=<FeatureTypeEnum.STRING: 'String'>)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    name=feature_group_name, # Replace None\n",
    "    feature_definitions=feature_definitions, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the defined Feature Group later in this lab, the actual creation of the Feature Group will take place in the processing job. Now let's move into the setup of the processing job to transform the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-2.'></a>\n",
    "# 2. Transform the dataset\n",
    "\n",
    "You will configure a SageMaker processing job to run a custom Python script to balance and transform the raw data into a format used by BERT model.\n",
    "\n",
    "Set the transformation parameters including the instance type, instance count, and train/validation/test split percentages. You can also choose whether you want to balance the dataset or not. In this case, you will balance the dataset to avoid class imbalance in the target variable, `sentiment`. \n",
    "\n",
    "Another important parameter of the model is the `max_seq_length`, which specifies the maximum length of the classified reviews for the RoBERTa model. If the sentence is shorter than the maximum length parameter, it will be padded. In another case, when the sentence is longer, it will be truncated from the right side.\n",
    "\n",
    "Since a smaller `max_seq_length` leads to faster training and lower resource utilization, you want to find the smallest power-of-2 that captures `100%` of our reviews.  For this dataset, the `100th` percentile is `115`.  However, it's best to stick with powers-of-2 when using BERT. So let's choose `128` as this is the smallest power-of-2 greater than `115`. You will see below how the shorter sentences will be padded to a maximum length.\n",
    "\n",
    "\n",
    "```\n",
    "mean        52.512374\n",
    "std         31.387048\n",
    "min          1.000000\n",
    "10%         10.000000\n",
    "20%         22.000000\n",
    "30%         32.000000\n",
    "40%         41.000000\n",
    "50%         51.000000\n",
    "60%         61.000000\n",
    "70%         73.000000\n",
    "80%         88.000000\n",
    "90%         97.000000\n",
    "100%       115.000000\n",
    "max        115.000000\n",
    "```\n",
    "\n",
    "![](images/distribution_num_words_per_review.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processing_instance_type='ml.c5.xlarge'\n",
    "processing_instance_count=1\n",
    "train_split_percentage=0.90\n",
    "validation_split_percentage=0.05\n",
    "test_split_percentage=0.05\n",
    "balance_dataset=True\n",
    "max_seq_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance and transform our data, we will use a scikit-learn-based processing job. This is essentially a generic Python processing job with scikit-learn pre-installed.  We can specify the version of scikit-learn we wish to use. Also pass the SageMaker execution role, processing instance type and instance count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': region},                             \n",
    "    max_runtime_in_seconds=7200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing job will be running the Python code from the file `src/prepare_data.py`. In the following exercise you will review the contents of the file and familiarize yourself with main parts of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-2'></a>\n",
    "### Exercise 2\n",
    "\n",
    "1. Open the file [src/prepare_data.py](src/prepare_data.py). Go through the comments to understand its content.\n",
    "2. Find and review the `convert_to_bert_input_ids()` function, which contains the RoBERTa `tokenizer` configuration.\n",
    "3. Complete method `encode_plus` of the RoBERTa `tokenizer`. Pass the `max_seq_length` as a value for the argument `max_length`. It defines a pad to a maximum length specified.\n",
    "4. Save the file [src/prepare_data.py](src/prepare_data.py) (with the menu command File -> Save Python File)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _This cell will take approximately 1-2 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f15c7ed9d4493cb4ac32e35450eb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689c5cd2cc1142a5bd14e9e5a5df0e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################\n",
      "Updated correctly!\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib\n",
    "sys.path.append('src/')\n",
    "\n",
    "# import the `prepare_data.py` module\n",
    "import prepare_data\n",
    "\n",
    "# reload the module if it has been previously loaded \n",
    "if 'prepare_data' in sys.modules:\n",
    "    importlib.reload(prepare_data)\n",
    "\n",
    "input_ids = prepare_data.convert_to_bert_input_ids(\"this product is great!\", max_seq_length)\n",
    "    \n",
    "updated_correctly = False\n",
    "\n",
    "if len(input_ids) != max_seq_length:\n",
    "    print('#######################################################################################################')\n",
    "    print('Please check that the function \\'convert_to_bert_input_ids\\' in the file src/prepare_data.py is complete.')\n",
    "    print('#######################################################################################################')\n",
    "    raise Exception('Please check that the function \\'convert_to_bert_input_ids\\' in the file src/prepare_data.py is complete.')\n",
    "else:\n",
    "    print('##################')\n",
    "    print('Updated correctly!')\n",
    "    print('##################')\n",
    "\n",
    "    updated_correctly = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the results of tokenization for the given example (*\\\"this product is great!\\\"*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9226, 1152, 16, 372, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of the sequence: 128\n"
     ]
    }
   ],
   "source": [
    "input_ids = prepare_data.convert_to_bert_input_ids(\"this product is great!\", max_seq_length)\n",
    "\n",
    "print(input_ids)\n",
    "print('Length of the sequence: {}'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the processing job with the custom script passing defined above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-07-04-02-37-10-071\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://dlai-practical-data-science/data/raw/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/input/code/prepare_data.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'sentiment-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-train', 'LocalPath': '/opt/ml/processing/output/sentiment/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'sentiment-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-validation', 'LocalPath': '/opt/ml/processing/output/sentiment/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'sentiment-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-test', 'LocalPath': '/opt/ml/processing/output/sentiment/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "if (updated_correctly):\n",
    "\n",
    "    processor.run(code='src/prepare_data.py',\n",
    "              inputs=[\n",
    "                    ProcessingInput(source=raw_input_data_s3_uri,\n",
    "                                    destination='/opt/ml/processing/input/data/',\n",
    "                                    s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                    ProcessingOutput(output_name='sentiment-train',\n",
    "                                     source='/opt/ml/processing/output/sentiment/train',\n",
    "                                     s3_upload_mode='EndOfJob'),\n",
    "                    ProcessingOutput(output_name='sentiment-validation',\n",
    "                                     source='/opt/ml/processing/output/sentiment/validation',\n",
    "                                     s3_upload_mode='EndOfJob'),\n",
    "                    ProcessingOutput(output_name='sentiment-test',\n",
    "                                     source='/opt/ml/processing/output/sentiment/test',\n",
    "                                     s3_upload_mode='EndOfJob')\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--balance-dataset', str(balance_dataset),\n",
    "                         '--max-seq-length', str(max_seq_length),                         \n",
    "                         '--feature-store-offline-prefix', str(feature_store_offline_prefix),\n",
    "                         '--feature-group-name', str(feature_group_name)                         \n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)\n",
    "\n",
    "else:\n",
    "    print('#######################################')\n",
    "    print('Please update the code correctly above.')\n",
    "    print('#######################################')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the information about the processing jobs using the `describe` function. The result is in dictionary format. Let's pull the processing job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name: sagemaker-scikit-learn-2021-07-04-02-37-10-071\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "print('Processing job name: {}'.format(scikit_processing_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-3'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Pull the processing job status from the processing job description.\n",
    "\n",
    "**Instructions**: Print the keys of the processing job description dictionary, choose the one related to the status of the processing job and print the value of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ProcessingInputs', 'ProcessingOutputConfig', 'ProcessingJobName', 'ProcessingResources', 'StoppingCondition', 'AppSpecification', 'Environment', 'RoleArn', 'ProcessingJobArn', 'ProcessingJobStatus', 'ProcessingEndTime', 'ProcessingStartTime', 'LastModifiedTime', 'CreationTime', 'ResponseMetadata'])\n",
      "Processing job status: Completed\n"
     ]
    }
   ],
   "source": [
    "print(processor.jobs[-1].describe().keys())\n",
    "\n",
    "### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "scikit_processing_job_status = processor.jobs[-1].describe()['ProcessingJobStatus'] # Replace None\n",
    "### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "print('Processing job status: {}'.format(scikit_processing_job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the created processing job in the AWS console.\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- notice that you are in the section `Amazon SageMaker` -> `Processing jobs`\n",
    "- check the name of the processing job, its status and other available information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2021-07-04-02-37-10-071\">processing job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">processing job</a></b>'.format(region, scikit_processing_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for about 5 minutes to review the CloudWatch Logs. You may open the file [src/prepare_data.py](src/prepare_data.py) again and examine the outputs of the code in the CloudWatch logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2021-07-04-02-37-10-071;streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> after about 5 minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> after about 5 minutes</b>'.format(region, scikit_processing_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the completion of the processing job you can also review the output in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/?region=us-east-1&tab=overview\">S3 output data</a> after the processing job has completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 output data</a> after the processing job has completed</b>'.format(bucket, scikit_processing_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the processing job to complete.\n",
    "\n",
    "### _This cell will take approximately 15 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!CPU times: user 25.3 ms, sys: 0 ns, total: 25.3 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=scikit_processing_job_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please wait until ^^ Processing Job ^^ completes above_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the transformed and balanced data in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-train\n",
      "s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-validation\n",
      "s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-test\n"
     ]
    }
   ],
   "source": [
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'sentiment-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'sentiment-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'sentiment-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-04 02:50:22    4874470 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-04 02:50:22     271210 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-04 02:50:23     273389 part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data into the folder `balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n",
      "download: s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n",
      "download: s3://sagemaker-us-east-1-509973781953/sagemaker-scikit-learn-2021-07-04-02-37-10-071/output/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $processed_train_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-train/\n",
    "!aws s3 cp $processed_validation_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-validation/\n",
    "!aws s3 cp $processed_test_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the training, validation and test data outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "10865\t0\t1\t[0, 713, 3588, 16, 11962, 8, 939, 40, 3568, 24, 7, 173, 4, 939, 129, 1276, 7, 489, 24, 142, 939, 300, 5, 1392, 425, 8, 41, 943, 843, 135, 160, 4, 5, 5780, 16, 11962, 1437, 53, 10, 410, 828, 8372, 1437, 24, 18, 10, 6966, 2089, 11, 10, 2927, 4, 5, 3318, 16, 1531, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tThis dress is cute and i will wear it to work. i only decided to keep it because i got the sale price and an additional 40 percent off. the print is cute  but a little bit odd  it's a swimmer in a cap. the tie is fun.\t2021-07-04T02:45:08Z\n",
      "9387\t0\t1\t[0, 100, 437, 70, 81, 42, 2904, 1437, 53, 42, 15203, 16, 1341, 7992, 73, 642, 1438, 24382, 1437, 8, 15, 162, 939, 619, 101, 24, 45422, 66, 8, 70, 47, 192, 16, 15503, 3512, 4, 939, 657, 24, 111, 53, 939, 206, 47, 240, 103, 6958, 13, 42, 7, 356, 63, 275, 4, 15, 162, 1437, 10, 23204, 15203, 74, 2532, 28, 357, 4, 939, 524, 7050, 219, 4, 53, 939, 17672, 2845, 4, 63, 269, 11962, 328, 114, 47, 214, 26647, 87, 195, 108, 306, 113, 218, 75, 21587, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\t\"I'm all over this trend  but this vest is quite thick/poufy  and on me i feel like it triangles out and all you see is furball. i love it - but i think you need some height for this to look its best. on me  a sweater vest would perhaps be better. i am chesty. but i cant decide. its really cute! if you're taller than 5'4\"\" don't hesitate.\"\t2021-07-04T02:45:08Z\n",
      "8007\t0\t1\t[0, 29287, 16170, 3588, 1437, 4613, 10199, 1437, 182, 157, 156, 1437, 95, 1237, 269, 650, 328, 98, 5074, 1437, 939, 770, 42, 3588, 13, 10, 780, 515, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tBeautiful dress  wonderful fabric  very well made  just runs really small! so sad  i wanted this dress for a special event!\t2021-07-04T02:45:08Z\n",
      "8098\t-1\t0\t[0, 100, 2854, 19, 5, 986, 37102, 14, 5, 10199, 8, 1254, 32, 2721, 1437, 53, 19, 5, 1481, 853, 4506, 31, 5, 5397, 1902, 8, 3668, 117, 12365, 4558, 1437, 127, 3989, 300, 4940, 685, 8, 939, 1415, 1307, 4, 5, 2249, 11, 5933, 9, 5, 3588, 8, 9215, 269, 18523, 162, 1437, 25, 157, 4, 3544, 372, 13, 6690, 1437, 142, 47, 356, 101, 110, 59, 290, 112, 73, 176, 377, 6992, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI agree with the previous reviewer that the fabric and details are beautiful  but with the shirring from the neckline and absolutely no fitting anywhere  my shape got totally lost and i looked huge. the difference in length of the dress and slip really bothered me  as well. possibly great for pregnancy  because you look like your about 8 1/2 months anyway.\t2021-07-04T02:45:08Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "9393\t1\t2\t[0, 16587, 42, 6013, 4, 24, 18, 2579, 8, 7082, 8, 1326, 372, 19, 70, 6134, 9, 17539, 6806, 4, 24, 18, 182, 36341, 114, 47, 33, 10, 1084, 22580, 4, 24, 18, 3793, 1437, 8, 372, 8089, 4, 269, 2579, 13, 2428, 9881, 1326, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tLove this tank. it's nice and loose and looks great with all kinds of bottoms. it's very forgiving if you have a midsection. it's soft  and great colors. really nice for spring casual looks.\t2021-07-04T02:45:08Z\n",
      "15362\t0\t1\t[0, 100, 437, 2333, 10, 231, 11, 14442, 8, 5, 231, 259, 2564, 8578, 4, 7082, 53, 202, 34203, 10, 410, 251, 13, 10, 28087, 5224, 4, 939, 21, 13865, 19, 24, 546, 10, 410, 6664, 35187, 8, 145, 98, 7082, 223, 5, 3701, 70, 127, 34904, 969, 479, 939, 362, 24, 124, 7, 5, 1400, 8, 71, 2600, 5, 6173, 259, 1276, 7, 860, 5, 2735, 10070, 734, 3056, 1533, 14442, 423, 939, 1249, 62, 2159, 10, 1836, 132, 4716, 1459, 16506, 8, 939, 437, 195, 108, 466, 845, 5, 132, 6701, 8, 4716, 1459, 202, 56, 5, 7082, 5660, 356, 7, 5, 299, 53, 258, 2564, 203, 357, 223, 5, 3701, 98, 21152, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\t\"I'm usually a 6 in dresses and the 6 here fit okay. loose but still flattering a little long for a sundress. i was unhappy with it looking a little frumpy and being so loose under the arms all my bras showed . i took it back to the store and after reading the reviews here decided to try the smaller sizes...well multiple dresses later i ended up buying a size 2 petite!!! and i'm 5'9\"\". the 2 reg and petite still had the loose gathering look to the top but both fit much better under the arms so tha\"\t2021-07-04T02:45:08Z\n",
      "5312\t-1\t0\t[0, 100, 2638, 5, 1114, 9, 42, 3588, 8, 21, 2818, 24, 74, 28, 966, 5, 34241, 425, 6694, 13, 99, 269, 16, 10, 13178, 11458, 3588, 4, 9574, 1437, 42, 3588, 2039, 5, 2458, 4, 11, 5, 3493, 42, 3588, 1326, 101, 5, 26348, 32, 13504, 2440, 8, 103, 2345, 9, 1104, 4, 157, 1437, 24, 16, 45, 1104, 50, 21247, 1173, 23, 70, 53, 1928, 6907, 328, 4520, 1928, 6907, 328, 190, 5, 1109, 6184, 15, 5, 13504, 3618, 16, 888, 6907, 4, 1307, 10208, 4, 127, 97, 696, 16, 19, 5, 5933, 9, 42, 3588, 4, 5, 8194, 161, 14, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI loved the idea of this dress and was hoping it would be worth the astronomical price tag for what really is a cotton jersey dress. unfortunately  this dress missed the mark. in the pictures this dress looks like the fabrics are navy blue and some sort of white. well  it is not white or whitish at all but baby pink! bright baby pink! even the light pattern on the navy background is actually pink. huge disappointment. my other issue is with the length of this dress. the description says that the\t2021-07-04T02:45:08Z\n",
      "3691\t1\t2\t[0, 100, 2162, 42, 128, 8498, 571, 3355, 108, 7, 3568, 25, 10, 2608, 21592, 3588, 4, 24, 269, 2564, 5, 1087, 12846, 5, 29716, 10199, 13596, 15481, 25, 939, 2307, 53, 21, 202, 34203, 137, 939, 300, 2514, 25, 157, 4, 89, 16, 117, 169, 939, 74, 888, 3581, 11, 42, 3588, 4, 24, 74, 28, 169, 350, 2131, 328, 24, 16, 11962, 19, 16701, 5933, 10317, 8, 939, 356, 556, 7, 21805, 24, 396, 5, 18951, 220, 2608, 4, 939, 74, 6329, 3568, 10, 4716, 1459, 5933, 8, 2162, 5, 4761, 8, 600, 24, 21, 1181, 1437, 5, 5933, 21, 45, 11789, 8, 1682, 24, 31, 145, 7, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI bought this 'nightgown' to wear as a winter maternity dress. it really fit the bill!! the knit fabric stretched nicely as i grew but was still flattering before i got larger as well. there is no way i would actually sleep in this dress. it would be way too hot! it is cute with calf length boots and i look forward to rocking it without the belly next winter. i would normally wear a petite length and bought the medium and though it was longer  the length was not awkward and kept it from being to\t2021-07-04T02:45:08Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id\tsentiment\tlabel_id\tinput_ids\treview_body\tdate\n",
      "12004\t-1\t0\t[0, 100, 437, 5779, 11, 209, 9304, 1437, 61, 1415, 98, 2721, 804, 8, 190, 33, 12189, 328, 11, 621, 5, 6184, 8, 8089, 2025, 75, 25, 2579, 1437, 5, 10199, 16, 350, 7174, 1437, 8, 5, 9304, 356, 6162, 4, 5, 12189, 32, 1256, 16762, 4, 5, 2564, 965, 75, 205, 1169, 4, 939, 2740, 62, 36, 118, 437, 227, 10070, 43, 8, 51, 214, 202, 45, 25, 3041, 219, 25, 939, 1017, 33, 6640, 4, 209, 32, 350, 3349, 24363, 25, 18437, 9304, 13, 162, 7, 28, 15, 5, 8146, 59, 106, 1437, 98, 51, 214, 164, 124, 4, 114, 47, 907, 106, 4730, 59, 5, 7174, 10199, 8, 41783, 2564, 1437, 51, 429, 19958, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI'm disappointed in these pants  which looked so beautiful online and even have pockets! in person the pattern and colors aren't as nice  the fabric is too thin  and the pants look cheap. the pockets are pretty shallow. the fit isn't good either. i ordered up (i'm between sizes) and they're still not as flowy as i'd have liked. these are too pricy as lounge pants for me to be on the fence about them  so they're going back. if you buy them knowing about the thin fabric and snug fit  they might wo\t2021-07-04T02:45:08Z\n",
      "16700\t1\t2\t[0, 100, 657, 5, 2216, 1521, 8, 5, 2564, 4, 51, 32, 182, 34203, 4, 51, 429, 28, 10, 410, 350, 7992, 7, 3568, 11, 5, 1035, 53, 5, 1079, 9, 5, 76, 51, 40, 173, 66, 372, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI love the unique design and the fit. they are very flattering. they might be a little too thick to wear in the summer but the rest of the year they will work out great.\t2021-07-04T02:45:08Z\n",
      "8121\t1\t2\t[0, 2387, 676, 19, 209, 33, 57, 372, 8, 5, 8089, 32, 190, 55, 2721, 11, 621, 328, 51, 214, 1365, 7, 3588, 62, 13, 173, 8, 164, 66, 8, 1365, 7, 3588, 159, 13, 7209, 66, 15, 5, 12729, 4, 939, 524, 3700, 10, 1836, 1105, 12, 2881, 8, 389, 18, 2564, 162, 101, 10, 19015, 4, 939, 67, 3700, 3568, 4716, 1459, 53, 218, 75, 101, 5, 95, 12, 23444, 12, 627, 12, 3153, 459, 847, 9, 209, 98, 939, 3584, 106, 11, 1675, 8, 3568, 106, 25, 9304, 4, 939, 64, 542, 12, 438, 5865, 106, 19, 43111, 73, 21831, 5641, 50, 489, 5, 1457, 36201, 8, 51, 478, 162, 6683, 25, 455, 12, 16096, 9304, 33755, 2, 1, 1, 1, 1, 1, 1, 1]\tMy experience with these have been great and the colors are even more beautiful in person! they're easy to dress up for work and going out and easy to dress down for hanging out on the weekends. i am typically a size 31-32 and 30's fit me like a glove. i also typically wear petite but don't like the just-above-the-ankle cut of these so i purchased them in regular and wear them as pants. i can un-cuff them with heals/wedges or keep the double cuff and they hit me perfectly as full-length pants wi\t2021-07-04T02:45:08Z\n",
      "13506\t0\t1\t[0, 100, 300, 42, 11, 5, 2272, 3195, 4, 24, 18, 10, 2721, 3195, 1437, 3793, 7, 5, 2842, 4, 939, 2854, 19, 97, 34910, 14, 24, 1302, 182, 15651, 4, 939, 218, 75, 1508, 3841, 12, 16008, 19266, 127, 24043, 11837, 1437, 98, 14, 18, 45, 5, 696, 4, 5, 936, 16, 14, 10, 4, 24, 1237, 1307, 111, 939, 300, 5, 650, 8, 24, 6966, 29, 15, 162, 4, 939, 3805, 7, 422, 227, 650, 8, 4761, 11, 13657, 111, 182, 5425, 7, 33, 10, 650, 28, 98, 380, 15, 162, 8, 741, 4, 5, 3369, 2629, 213, 190, 723, 87, 51, 356, 15, 5, 1421, 111, 51, 283, 62, 7, 5, 299, 9, 127, 13977, 4, 190, 19, 239, 12, 2739, 15346, 9304, 1437, 2]\tI got this in the green color. it's a beautiful color  soft to the touch. i agree with other reviewers that it seems very delicate. i don't mind dry-cleaning my sweaters  so that's not the issue. the problem is that a. it runs huge - i got the small and it swims on me. i tend to run between small and medium in tops - very unusual to have a small be so big on me and b. the slits go even higher than they look on the model - they come up to the top of my waist. even with high-waisted pants  the sli\t2021-07-04T02:45:08Z\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.'></a>\n",
    "# 3. Query the Feature Store\n",
    "In addition to transforming the data and saving in S3 bucket, the processing job populates the feature store with the transformed and balanced data.  Let's query this data using Amazon Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.1.'></a>\n",
    "### 3.1. Export training, validation, and test datasets from the Feature Store\n",
    "\n",
    "Here you will do the export only for the training dataset, as an example. \n",
    "\n",
    "Use `athena_query()` function to create an Athena query for the defined above Feature Group. Then you can pull the table name of the Amazon Glue Data Catalog table which is auto-generated by Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue Catalog table name: reviews-feature-group-1625365556-1625366673\n",
      "Running query: \n",
      "    SELECT date,\n",
      "        review_id,\n",
      "        sentiment, \n",
      "        label_id,\n",
      "        input_ids,\n",
      "        review_body\n",
      "    FROM \"reviews-feature-group-1625365556-1625366673\" \n",
      "    WHERE split_type='train' \n",
      "    LIMIT 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_query = feature_group.athena_query()\n",
    "\n",
    "feature_store_table = feature_store_query.table_name\n",
    "\n",
    "query_string = \"\"\"\n",
    "    SELECT date,\n",
    "        review_id,\n",
    "        sentiment, \n",
    "        label_id,\n",
    "        input_ids,\n",
    "        review_body\n",
    "    FROM \"{}\" \n",
    "    WHERE split_type='train' \n",
    "    LIMIT 5\n",
    "\"\"\".format(feature_store_table)\n",
    "\n",
    "print('Glue Catalog table name: {}'.format(feature_store_table))\n",
    "print('Running query: {}'.format(query_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the S3 location for the query results.  This allows us to re-use the query results for future queries if the data has not changed.  We can even share this S3 location between team members to improve query performance for common queries on data that does not change often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-509973781953/query_results/reviews-feature-store-1625365556/\n"
     ]
    }
   ],
   "source": [
    "output_s3_uri = 's3://{}/query_results/{}/'.format(bucket, feature_store_offline_prefix)\n",
    "print(output_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-4'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Query the feature store.\n",
    "\n",
    "**Instructions**: Use `feature_store_query.run` function passing the constructed above query string and the location of the output S3 bucket.\n",
    "\n",
    "```python\n",
    "feature_store_query.run(\n",
    "    query_string=..., # query string\n",
    "    output_location=... # location of the output S3 bucket\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query.run(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    query_string=query_string, # Replace None\n",
    "    output_location=output_s3_uri # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")\n",
    "\n",
    "feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label_id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04T02:45:08Z</td>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 713, 3588, 16, 11962, 8, 939, 40, 3568, 24, 7, 173, 4, 939, 129, 1276, 7, 489, 24, 142, 939,...</td>\n",
       "      <td>This dress is cute and i will wear it to work. i only decided to keep it because i got the sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-04T02:45:08Z</td>\n",
       "      <td>12184</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 100, 770, 7, 657, 42, 299, 1437, 53, 56, 132, 538, 1272, 19, 24, 4, 78, 1437, 24, 18, 45, 13...</td>\n",
       "      <td>I wanted to love this top  but had 2 major problems with it. first  it's not for short girls. i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-04T02:45:08Z</td>\n",
       "      <td>9491</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 713, 6399, 16, 98, 650, 4, 939, 524, 45, 686, 54, 376, 62, 19, 5, 39328, 53, 47, 33, 7, 213,...</td>\n",
       "      <td>This shirt is so small. i am not sure who came up with the sizing but you have to go up several ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-04T02:45:08Z</td>\n",
       "      <td>3720</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 100, 2638, 42, 299, 804, 53, 24, 2035, 182, 45218, 1329, 61, 1179, 10, 1275, 3794, 25, 7, 14...</td>\n",
       "      <td>I loved this top online but it arrived very wrinkled which raised a red flag as to how it would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-04T02:45:08Z</td>\n",
       "      <td>3526</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 2264, 10, 372, 465, 328, 42, 16, 98, 1365, 7, 3588, 62, 50, 159, 8, 10, 182, 3473, 2564, 8, ...</td>\n",
       "      <td>What a great find! this is so easy to dress up or down and a very comfortable fit and fabric. th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  review_id  sentiment  label_id  \\\n",
       "0  2021-07-04T02:45:08Z      10865          0         1   \n",
       "1  2021-07-04T02:45:08Z      12184          0         1   \n",
       "2  2021-07-04T02:45:08Z       9491         -1         0   \n",
       "3  2021-07-04T02:45:08Z       3720         -1         0   \n",
       "4  2021-07-04T02:45:08Z       3526          1         2   \n",
       "\n",
       "                                                                                             input_ids  \\\n",
       "0  [0, 713, 3588, 16, 11962, 8, 939, 40, 3568, 24, 7, 173, 4, 939, 129, 1276, 7, 489, 24, 142, 939,...   \n",
       "1  [0, 100, 770, 7, 657, 42, 299, 1437, 53, 56, 132, 538, 1272, 19, 24, 4, 78, 1437, 24, 18, 45, 13...   \n",
       "2  [0, 713, 6399, 16, 98, 650, 4, 939, 524, 45, 686, 54, 376, 62, 19, 5, 39328, 53, 47, 33, 7, 213,...   \n",
       "3  [0, 100, 2638, 42, 299, 804, 53, 24, 2035, 182, 45218, 1329, 61, 1179, 10, 1275, 3794, 25, 7, 14...   \n",
       "4  [0, 2264, 10, 372, 465, 328, 42, 16, 98, 1365, 7, 3588, 62, 50, 159, 8, 10, 182, 3473, 2564, 8, ...   \n",
       "\n",
       "                                                                                           review_body  \n",
       "0  This dress is cute and i will wear it to work. i only decided to keep it because i got the sale ...  \n",
       "1  I wanted to love this top  but had 2 major problems with it. first  it's not for short girls. i'...  \n",
       "2  This shirt is so small. i am not sure who came up with the sizing but you have to go up several ...  \n",
       "3  I loved this top online but it arrived very wrinkled which raised a red flag as to how it would ...  \n",
       "4  What a great find! this is so easy to dress up or down and a very comfortable fit and fabric. th...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 100)\n",
    "\n",
    "df_feature_store = feature_store_query.as_dataframe()\n",
    "df_feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Feature Store in SageMaker Studio\n",
    "\n",
    "![](images/sm_studio_extensions_featurestore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.2.'></a>\n",
    "### 3.2. Export TSV from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output as a TSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_store.to_csv('./feature_store_export.tsv',\n",
    "                        sep='\\t',\n",
    "                        index=False,\n",
    "                        header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\treview_id\tsentiment\tlabel_id\tinput_ids\treview_body\n",
      "2021-07-04T02:45:08Z\t10865\t0\t1\t[0, 713, 3588, 16, 11962, 8, 939, 40, 3568, 24, 7, 173, 4, 939, 129, 1276, 7, 489, 24, 142, 939, 300, 5, 1392, 425, 8, 41, 943, 843, 135, 160, 4, 5, 5780, 16, 11962, 1437, 53, 10, 410, 828, 8372, 1437, 24, 18, 10, 6966, 2089, 11, 10, 2927, 4, 5, 3318, 16, 1531, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tThis dress is cute and i will wear it to work. i only decided to keep it because i got the sale price and an additional 40 percent off. the print is cute  but a little bit odd  it's a swimmer in a cap. the tie is fun.\n",
      "2021-07-04T02:45:08Z\t12184\t0\t1\t[0, 100, 770, 7, 657, 42, 299, 1437, 53, 56, 132, 538, 1272, 19, 24, 4, 78, 1437, 24, 18, 45, 13, 765, 1972, 4, 939, 437, 129, 195, 108, 6764, 8, 24, 21, 10, 746, 7319, 15, 162, 10301, 11036, 4, 5, 21764, 58, 169, 350, 251, 8, 5, 27595, 147, 5, 23204, 8, 3723, 2911, 783, 972, 478, 235, 11, 5, 1692, 9, 127, 17507, 1437, 61, 21, 23213, 29747, 24203, 4, 200, 1437, 5, 10521, 10199, 21, 182, 7174, 8, 24282, 6162, 546, 4, 117, 19078, 15, 3357, 42, 65, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI wanted to love this top  but had 2 major problems with it. first  it's not for short girls. i'm only 5' tall and it was a total mess on me proportion wise. the sleeves were way too long and the seam where the sweater and peplum meet hit right in the middle of my butt  which was terribly unflattering. second  the grey fabric was very thin and kinda cheap looking. no regrets on returning this one.\n",
      "2021-07-04T02:45:08Z\t9491\t-1\t0\t[0, 713, 6399, 16, 98, 650, 4, 939, 524, 45, 686, 54, 376, 62, 19, 5, 39328, 53, 47, 33, 7, 213, 62, 484, 10070, 7, 120, 10, 3473, 2564, 4, 218, 75, 3844, 110, 86, 15, 42, 6399, 4, 215, 10, 9208, 1437, 24, 115, 33, 57, 182, 11962, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tThis shirt is so small. i am not sure who came up with the sizing but you have to go up several sizes to get a comfortable fit. don't waste your time on this shirt. such a shame  it could have been very cute.\n",
      "2021-07-04T02:45:08Z\t3720\t-1\t0\t[0, 100, 2638, 42, 299, 804, 53, 24, 2035, 182, 45218, 1329, 61, 1179, 10, 1275, 3794, 25, 7, 141, 24, 74, 356, 71, 2498, 24, 4, 2551, 239, 4861, 4, 67, 1437, 399, 75, 657, 5, 2564, 4, 3195, 16, 11577, 53, 1374, 45, 10, 2378, 9, 42, 299, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\tI loved this top online but it arrived very wrinkled which raised a red flag as to how it would look after wearing it. seemed high maintenance. also  didn't love the fit. color is vibrant but overall not a fan of this top.\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ./feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload TSV to the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./feature_store_export.tsv to s3://sagemaker-us-east-1-509973781953/feature_store/feature_store_export.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./feature_store_export.tsv s3://$bucket/feature_store/feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the file in the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-04 03:08:43       3982 feature_store/feature_store_export.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive s3://$bucket/feature_store/feature_store_export.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-3.3.'></a>\n",
    "### 3.3. Check that the dataset in the Feature Store is balanced by sentiment\n",
    "\n",
    "Now you can setup an Athena query to check that the stored dataset is balanced by the target class `sentiment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-5'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Write an SQL query to count the total number of the reviews per `sentiment` stored in the Feature Group.\n",
    "\n",
    "**Instructions**: Pass the SQL statement of the form \n",
    "\n",
    "```sql\n",
    "SELECT category_column, COUNT(*) AS new_column_name\n",
    "FROM table_name\n",
    "GROUP BY category_column\n",
    "```\n",
    "\n",
    "into the variable `query_string_count_by_sentiment`. Here you would need to use the column `sentiment` and give a name `count_reviews` to the new column with the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query_2 = feature_group.athena_query()\n",
    "\n",
    "# Replace all None\n",
    "### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "query_string_count_by_sentiment = \"\"\"\n",
    "SELECT output_s3_uri, COUNT(*) AS count_reviews\n",
    "FROM \"{}\"\n",
    "GROUP BY sentiment\n",
    "\"\"\".format(feature_store_table)\n",
    "### END SOLUTION - DO NOT delete this comment for grading purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-6'></a>\n",
    "### Exercise 6\n",
    "\n",
    "Query the feature store.\n",
    "\n",
    "**Instructions**: Use `run` function of the Feature Store query, passing the new query string `query_string_count_by_sentiment`. The output S3 bucket will remain unchanged. You can follow the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to execute query 285f5716-7a5e-49d9-b662-d087ea5a1e4c.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to execute query 285f5716-7a5e-49d9-b662-d087ea5a1e4c",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d3632785d888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeature_store_query_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_count_by_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_store_query_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf_count_by_sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/feature_store/feature_group.py\u001b[0m in \u001b[0;36mas_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;34mf\"Current query {self._current_query_execution_id} is still being executed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 )\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to execute query {self._current_query_execution_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         output_filename = os.path.join(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to execute query 285f5716-7a5e-49d9-b662-d087ea5a1e4c"
     ]
    }
   ],
   "source": [
    "feature_store_query_2.run(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    query_string=query_string_count_by_sentiment, # Replace None\n",
    "    output_location=output_s3_uri # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")\n",
    "\n",
    "feature_store_query_2.wait()\n",
    "\n",
    "df_count_by_sentiment = feature_store_query_2.as_dataframe()\n",
    "df_count_by_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c2w1-ex-7'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Visualize the result of the query in the bar plot, showing the count of the reviews by sentiment value.\n",
    "\n",
    "**Instructions**: Pass the resulting data frame `df_count_by_sentiment` into the `barplot` function of the `seaborn` library.\n",
    "\n",
    "```python\n",
    "sns.barplot(\n",
    "    data=..., \n",
    "    x='...', \n",
    "    y='...',\n",
    "    color=\"blue\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    data=None, # Replace None\n",
    "    x=None, # Replace None\n",
    "    y=None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    color=\"blue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the notebook and `prepare_data.py` file into S3 bucket for grading purposes.\n",
    "\n",
    "**Note**: you may need to save the file before the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./C2_W1_Assignment.ipynb s3://$bucket/C2_W1_Assignment_Learner.ipynb\n",
    "!aws s3 cp ./src/prepare_data.py s3://$bucket/src/C2_W1_prepare_data_Learner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to the main lab window and click on `Submit` button (see the `Finish the lab` section of the instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
